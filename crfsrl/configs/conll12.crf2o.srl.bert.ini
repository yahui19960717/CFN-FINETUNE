[Data]
bert = 'hfl/chinese-roberta-wwm-ext-large'

[Network]
n_bert_layers = 4
mix_dropout = .0
bert_pooling = 'mean'
encoder_dropout = .1
n_edge_mlp = 500
n_sib_mlp = 100
n_role_mlp = 100
mlp_dropout = .1

[Optimizer]
lr = 5e-5
lr_rate = 20
mu = .9
nu = .9
eps = 1e-12
weight_decay = 0
clip = 5.0
min_freq = 2
fix_len = 20
epochs = 60
warmup = 0.1
update_steps = 1
batch_size = 100